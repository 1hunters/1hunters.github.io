<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Chen Tang - The Chinese University of Hong Kong</title>

  <!-- Enable responsive viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
  <!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

  <!-- Le styles -->
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/font-awesome.min.css" rel="stylesheet">
  <link href="css/syntax.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">

</head>

<body>

  <div class="ccontent">
    <div class="content heading">
      <div id="contact-list" class="div_left">
        <img src="./assets/img/photo.jpg" class="img-circle" />
      </div>
      <div id="contact-list" class="div_right">
        <h1>Chen Tang 唐辰</h1>
        <p>Ph.D. student, MMLab, The Chinese University of Hong Kong</p>
        <!-- <p>Department of Computer Science and Technology, Tsinghua University</p> -->
        <p>chentang [AT] link.cuhk.edu.hk / tangchen18 [AT] outlook.com</p>
        <p> [<a href="https://scholar.google.com/citations?user=WjZUs1AAAAAJ&hl=en" target="_blank">Google
            Scholar</a>]
          [<a href="https://openreview.net/profile?id=~Chen_Tang3" target="_blank">OpenReview</a>]
          [<a href="https://www.linkedin.com/in/chen-tang-99733025b/" target="_blank">LinkedIn</a>]
          [<a href="https://github.com/1hunters" target="_blank">GitHub</a>]</p>
      </div>
    </div>
    
    <div class="Bio">
      <header>
        <h1 margin="0px" style="color:#008B8B;"> Short Bio</h1>
      </header>
      <p>
        I am currently a Ph.D. student at the <a href="https://mmlab.ie.cuhk.edu.hk" target="_blank">Multimedia Laboratory
          </a> (MMLab), <a href="https://www.cuhk.edu.hk" target="_blank">The Chinese University of Hong Kong</a>, co-supervised by Prof. <a href="https://wlouyang.github.io/" target="_blank">Wanli Ouyang</a> and Prof. <a href="https://xyue.io/" target="_blank">Xiangyu Yue</a>. 
        Before joining CUHK, I was a research assistant in the <a href="https://www.cs.tsinghua.edu.cn/csen/"
          target="_blank">Department of Computer Science and Technology</a> at <a href="https://www.tsinghua.edu.cn/en/"
          target="_blank">Tsinghua University</a>, working with Prof. <a href="https://www.cs.tsinghua.edu.cn/csen/info/1306/4336.htm" target="_blank">Wenwu
          Zhu</a>, Prof. <a href="https://www.mmlab.top" target="_blank">Zhi
          Wang</a>, and Prof. <a href="https://mengyuan404.github.io" target="_blank">Yuan Meng</a>. 
        I received my Master's degree in Computer Technology from Tsinghua University, advised by Prof. <a
          href="https://www.mmlab.top" target="_blank">Zhi Wang</a>. 
        I won the Distinguished Master's Thesis Award of Tsinghua. 
      </p> 
      <p>
        I was a visiting student at AIoT Lab, <a href="https://air.tsinghua.edu.cn/en/" target="_blank">Institute for AI Industry Research (AIR)</a> at Tsinghua University, working with Prof. <a href="https://yuanchun-li.github.io/" target="_blank">Yuanchun Li</a> and Prof. <a href="https://yunxinliu.github.io/" target="_blank">Yunxin Liu</a>. 
        Prior to that, I spent a wonderful year as a research intern in the <a
          href="https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/"
          target="_blank">System and Networking Research Group</a> at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target="_blank">Microsoft Research Asia</a>, working with Dr. <a href="https://www.microsoft.com/en-us/research/people/lzhani/" target="_blank">Li Lyna Zhang</a>. 
        I am a member of the <a href="https://sagroups.ieee.org/3161/members/" target="_blank">IEEE Digital Retina Systems Working Group (3161 WG)</a>, and have served as a reviewer for CVPR, ECCV, ICCV, ACM Multimedia, ICML, ICLR, NeurIPS, and AAAI. 
      </p>
      <p>
        My research interests are in AI for science, efficient learning, multimodal learning, and generative learning. Feel free to contact me if you are interested in my research. 
      </p>
    </div>

    <div class="News">
      <header>
        <h1 style="color:#008B8B;"> News</h1>
      </header>
      <ul style="padding-left:2em">
        <li>
          <p>
            [2025/02] Three papers got accepted to CVPR 2025.
          </p>
        </li>
        <li>
          <p>
            [2024/12] Two papers got accepted to IEEE TMC and AAAI 2025, respectively.
          </p>
        </li>
        <li>
          <p>
            [2024/07] One paper got accepted to ECCV 2024.
          </p>
        </li>
        <li>
          <p>
            [2024/02] Two papers got accepted to CVPR 2024 and ICLR 2024 PML4LRS Workshop, respectively.
          </p>
        </li>
        <li>
          <p>
            [2023/07] Two papers got accepted to ICCV 2023, ECML-PKDD 2023, and ACM Multimedia 2023, respectively.
          </p>
        </li>
        <li>
          <p>
            [2023/06] I earned my Master's degree along with the <strong>Distinguished Master's Thesis Award</strong>!
          </p>
        </li>
      </ul>
    </div>

    <div class="Publication">
      <header>
        <h1 id="pubs" style="color:#008B8B;"> Selected Publications and Preprints </h1>
        <a id="sortByYear" class="sort-link" onclick="sortPapers('year')">[Sord by year]</a>
        <a id="sortByAuthor" class="sort-link" onclick="sortPapers('author')">[Sord by authorship]</a>
        <a id="fullPubs" class="sort-link" href="https://scholar.google.com/citations?user=WjZUs1AAAAAJ&hl=en" target="_blank">[Full Publications]</a> 
        (*: Equal contributions) 
      </header>
      <br>
      <ul style="padding-left:2em" id="paperList">

        <li data-year="2024" paper-weighting=1>
          <p class="paper"><strong>UniSTD: Towards Unified Spatio-Temporal Prediction across Diverse Disciplines</strong><br>
            <strong>Chen Tang</strong>, Xinzhu Ma, Encheng Su, Xiufeng Song, Xiaohong Liu, Wei-Hong Li, Lei Bai, Wanli Ouyang, Xiangyu Yue<br>
            <u>[CVPR'25]</u> <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2025
            [<a href="#">PDF</a>] 
          </p>
        </li>

        <li data-year="2025" paper-weighting=5>
          <p class="paper"><strong>EVOS: Efficient Implicit Neural Training via EVOlutionary Selector</strong><br>
            Weixiang Zhang, Shuzhao Xie, Chengwei Ren, Siyi Xie, <strong>Chen Tang</strong>, Shijia Ge, Mingzi Wang, Zhi Wang<br>
            <u>[CVPR'25]</u> <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2025
            [<a href="https://arxiv.org/pdf/2412.10153" target="_blank">PDF</a>] 
          </p>
        </li>

        <li data-year="2025" paper-weighting=3>
          <p class="paper"><strong>Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers</strong><br>
            Lei Chen, Yuan Meng, <strong>Chen Tang</strong>, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, Wenwu Zhu<br>
            <u>[CVPR'25]</u> <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2025
            [<a href="https://arxiv.org/pdf/2406.17343" target="_blank">PDF</a>] 
            [<a href="https://github.com/Juanerx/Q-DiT/" target="_blank">Code</a>]
          </p>
        </li>

        <li data-year="2025" paper-weighting=3>
          <p class="paper"><strong>JAQ: Joint Efficient Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration</strong><br>
            Mingzi Wang, Yuan Meng, <strong>Chen Tang</strong>, Weixiang Zhang, Yijian Qin, Yang Yao, Yingxin Li, Tongtong Feng, Xin Wang, Xun Guan, Zhi Wang, Wenwu Zhu<br>
            <u>[AAAI'25]</u> <em>The AAAI Conference on Artificial Intelligence</em>, 2025
            [<a href="https://arxiv.org/pdf/2501.05339" target="_blank">PDF</a>] 
            [<a href="https://github.com/wmz-opensource/JAQ/" target="_blank">Code</a>] 
          </p>
        </li>

        <li data-year="2024" paper-weighting=4>
          <p class="paper"><strong>Data-Aware Gradient Compression for FL in Communication-Constrained Mobile Computing</strong><br>
            Rongwei Lu, Yutong Jiang, Yinan Mao, <strong>Chen Tang</strong>, Bin Chen, Laizhong Cui, Zhi Wang<br>
            <u>[IEEE TMC]</u> <em>IEEE Transactions on Mobile Computing (TMC)</em>, 2024
            [<a href="https://arxiv.org/pdf/2311.07324" target="_blank">PDF</a>]  
            [<a href="https://github.com/Jiang-Yutong/DAGC" target="_blank">Code</a>]
          </p>
        </li>
        
        <li data-year="2024" paper-weighting=5>
          <p class="paper"><strong>Expansive Supervision for Neural Radiance Field</strong><br>
            Weixiang Zhang, Shuzhao Xie, Shijia Ge, Wei Yao, <strong>Chen Tang</strong>, Zhi Wang<br>
            <em>Preprint</em>, 2024
            [<a href="https://arxiv.org/pdf/2409.08056" target="_blank">PDF</a>] 
          </p>
        </li>

        <li data-year="2024" paper-weighting=1.5>
          <p class="paper"><strong>PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference</strong><br>
            Ye Li*, <strong>Chen Tang</strong>*, Yuan Meng, Jiajun Fan, Zenghao Chai, Xinzhu Ma, Zhi Wang, Wenwu Zhu<br>
            In submission to <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
            [<a href="https://arxiv.org/pdf/2407.05010" target="_blank">PDF</a>] 
            [<a href="https://github.com/ChildTang/PRANCE" target="_blank">Code</a>] 
          </p>
        </li>

        <li data-year="2024" paper-weighting=3>
          <p class="paper"><strong>MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation
          </strong><br>
            Shuzhao Xie, Weixiang Zhang, <strong>Chen Tang</strong>, Yunpeng Bai, Rongwei Lu, Shijia Ge, Zhi Wang<br>
            <u>[ECCV'24]</u> <em>European Conference on Computer Vision</em>, 2024
            [<a href="./assets/paper/ECCV_mesongs.pdf" target="_blank">PDF</a>] 
            [<a href="https://shuzhaoxie.github.io/data/24-eccv-mesongs-supp.pdf" target="_blank">Supp</a>] 
            [<a href="https://shuzhaoxie.github.io/mesongs" target="_blank">Project</a>] 
            [<a href="./assets/poster/ECCV24_MesonGS_poster.pdf" target="_blank">Poster</a>] 
            [<a href="https://github.com/ShuzhaoXie/MesonGS" target="_blank">Code</a>] 
          </p>
        </li>

        <li data-year="2024" paper-weighting=1>
          <p class="paper"><strong>RFQuant: Retraining-free Model Quantization via One-Shot Weight-Coupling Learning</strong><br>
            <strong>Chen Tang</strong>*, Yuan Meng*, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu
            Zhu<br>
            <u>[CVPR'24]</u> <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2024
            [<a href="./assets/paper/CVPR_retraining_free_quantization.pdf" target="_blank">PDF</a>] 
            [<a href="./assets/paper/CVPR_retraining_free_quantization_suppl.pdf" target="_blank">Supp</a>] 
            [<a href="./assets/poster/CVPR24_retraining_free_quantization_poster.pdf" target="_blank">Poster</a>] 
            [<a href="https://github.com/1hunters/retraining-free-quantization/" target="_blank">Code</a>] 
          </p>
        </li>

        <li data-year="2024" paper-weighting=4>
          <p class="paper"><strong>One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
              Deployments</strong><br>
            Ke Yi, Yuhui Xu, Heng Chang, <strong>Chen Tang</strong>, Yuan Meng, Tong Zhang, Jia Li<br>
            <em>Preprint</em>, 2024
            [<a href="https://arxiv.org/pdf/2405.20202" target="_blank">PDF</a>]
            [<a href="#" target="_blank">Code</a>]
          </p>
        </li>
        
        <li data-year="2024" paper-weighting=1.5>
          <p class="paper"><strong>TMPQ-DM: Joint Timestep Reduction and Quantization Precision
              Selection for Efficient Diffusion Models</strong><br>
            Haojun Sun*, <strong>Chen Tang</strong>*, Zhi Wang, Yuan Meng, Jingyan Jiang, Xinzhu Ma, Wenwu Zhu<br>
            <em>Preprint</em>, 2024
            [<a href="https://arxiv.org/pdf/2404.09532.pdf" target="_blank">PDF</a>]
            [<a
              href="https://mmlabsigs.notion.site/TMPQ-DM-Joint-Timestep-Reduction-and-Quantization-Precision-Selection-for-Efficient-Diffusion-Model-ea0af24bd3ef4929a1bc07c364ce4ac5?pvs=74"
              target="_blank">Project</a>]
            [<a href="https://github.com/sihouzi21c/TMPQ-DM" target="_blank">Code</a>]
          </p>
        </li>

        <li data-year="2024" paper-weighting=2>
          <p class="paper"><strong>STAR: Skeleton-aware Text-based 4D Avatar
              Generation with In-Network Motion Retargeting</strong><br>
            Zenghao Chai, <strong>Chen Tang</strong>, Yongkang Wong, Mohan Kankanhalli<br>
            In submission to <em>IEEE Transactions on Visualization and Computer Graphics (TVCG)</em>, 2024
            [<a href="https://arxiv.org/pdf/2406.04629" target="_blank">PDF</a>]
            [<a href="https://star-avatar.github.io/" target="_blank">Project</a>]
            [<a href="https://github.com/czh-98/STAR" target="_blank">Code</a>]
          </p>
        </li>

        <li data-year="2024" paper-weighting=7>
          <p class="paper"><strong>Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and
              Toolbox</strong><br>
            Yijun Liu, Yuan Meng, Fang Wu, Shenhao Peng, Hang Yao, Chaoyu Guan, <strong>Chen Tang</strong>, Xinzhu Ma,
            Zhi Wang, Wenwu Zhu<br>
            <em>Preprint</em>, 2024
            [<a href="https://arxiv.org/pdf/2406.12928" target="_blank">PDF</a>]
            [<a href="https://github.com/TsingmaoAI/MI-optimize" target="_blank">Toolbox</a>]
          </p>
        </li>

        <li data-year="2024" paper-weighting=3>
          <p class="paper"><strong>Investigating the Impact of Quantization on Adversarial Robustness</strong><br>
            Qun Li, Yuan Meng, <strong>Chen Tang</strong>, Jiacheng Jiang, Zhi Wang<br>
            <u>[ICLR PML4LRS'24]</u> <em>International Conference on Learning Representations (Workshop on Practical ML
              for Limited Resource Settings)</em>, 2024
            [<a href="https://arxiv.org/pdf/2404.05639.pdf" target="_blank">PDF</a>]
          </p>
        </li>

        <li data-year="2024" paper-weighting=1.5>
          <p class="paper"><strong>Breaking the Curse of Knowledge: Towards Effective Multimodal Recommendation using Knowledge Soft Integration</strong><br>
            Kai Ouyang*, <strong>Chen Tang</strong>*, Wenhao Zheng, Xiangjin Xie, Xuanji Xiao, Jian Dong, Hai-tao Zheng,
            Zhi Wang<br>
            In submission to <em>IEEE Transactions on Multimedia (TMM)</em>, 2024
            [<a href="https://arxiv.org/pdf/2305.07419.pdf" target="_blank">PDF</a>]
          </p>
        </li>

        <li data-year="2023" paper-weighting=3>
          <p class="paper"><strong>Click-aware Structure Transfer with Sample Weight Assignment for Post-Click
              Conversion Rate
              Estimation</strong><br>
            Kai Ouyang, Wenhao Zheng, <strong>Chen Tang</strong>, Xuanji Xiao, Hai-tao Zheng<br>
            <u>[ECML-PKDD'23]</u> <em>European Conference on Machine Learning and Principles and Practice of Knowledge
              Discovery in Databases</em>, 2023 [<a href="https://arxiv.org/pdf/2304.01169.pdf" target="_blank">PDF</a>]
            [<a href="https://github.com/OuyKai/CSTWA" target="_blank">Code</a>]
          </p>
        </li>

        <li data-year="2023" paper-weighting=1>
          <p class="paper"><strong>SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin
              Regularization</strong><br>
            <strong>Chen Tang</strong>, Kai Ouyang, Zenghao Chai, Yunpeng Bai, Yuan Meng, Zhi Wang, Wenwu Zhu<br>
            <u>[ACM MM'23]</u> <em>ACM International Conference on Multimedia</em>, 2023
            [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3611975" target="_blank">PDF</a>]
          </p>
        </li>

        <li data-year="2023" paper-weighting=1>
          <p class="paper"><strong>ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices</strong><br>
            <strong>Chen Tang</strong>*, Li Lyna Zhang*, Huiqiang Jiang, Jiahang
            Xu, Ting Cao, Quanlu Zhang, Yuqing Yang, Zhi Wang, Mao Yang<br>
            <u>[ICCV'23]</u> <em>International Conference on Computer Vision</em>, 2023
            [<a
              href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_ElasticViT_Conflict-aware_Supernet_Training_for_Deploying_Fast_Vision_Transformer_on_ICCV_2023_paper.pdf"
              target="_blank">PDF</a>]
            [<a
              href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Tang_ElasticViT_Conflict-aware_Supernet_ICCV_2023_supplemental.pdf"
              target="_blank">Supp</a>]
            [<a href="./assets/poster/ICCV23_ElasticViT_poster.pdf" target="_blank">Poster</a>]
            [<a href="https://github.com/microsoft/Moonlit/tree/main/ElasticViT" target="_blank">Code</a>]
          </p>
          </p>
        </li>

        <li data-year="2022" paper-weighting=1>
          <p class="paper"><strong>Mixed-Precision Network Quantization via Learned Layer-wise Importance</strong><br>
            <strong>Chen Tang</strong>*, Kai Ouyang*, Zhi Wang, Yifei Zhu, Wen Ji,
            Yaowei Wang, Wenwu Zhu<br>
            <u>[ECCV'22]</u> <em>European Conference on Computer Vision</em>, 2022
            [<a href="https://arxiv.org/pdf/2203.08368.pdf" target="_blank">PDF</a>]
            [<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136710260-supp.pdf"
              target="_blank">Supp</a>]
            [<a
              href="https://mmlabsigs.notion.site/Mixed-Precision-Neural-Network-Quantization-via-Learned-Layer-wise-Importance-18f8e075f20f4b9b99564c6d35a18744/"
              target="_blank">Project</a>]
            [<a href="./assets/poster/ECCV22_LIMPQ_poster.pdf" target="_blank">Poster</a>]
            [<a href="https://github.com/1hunters/LIMPQ" target="_blank">Code</a>]
          </p>
        </li>

        <li data-year="2022" paper-weighting=1>
          <p class="paper"><strong>Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference
              Approach</strong><br>
            <strong>Chen Tang</strong>, Haoyu Zhai, Kai Ouyang, Zhi Wang, Yifei Zhu, Wenwu Zhu<br>
            <u>[ACM MM'22]</u> <em>ACM International Conference on Multimedia</em>, 2022
            [<a href="https://arxiv.org/pdf/2204.09992.pdf" target="_blank">PDF</a>]
          </p>
        </li>

        <li data-year="2023" paper-weighting=3>
          <p class="paper"><strong>Social-aware Sparse Attention Network for Session-based Social
              Recommendation</strong><br>
            Kai Ouyang, Xianghong Xu, <strong>Chen Tang</strong>, Wang Chen, Hai-tao Zheng<br>
            <u>[Findings of EMNLP'22]</u> <em>Findings of Conference on Empirical Methods in Natural Language
              Processing</em>, 2022 [<a href="https://aclanthology.org/2022.findings-emnlp.159.pdf"
              target="_blank">PDF</a>]
          </p>
        </li>
      </ul>
    </div>

    <div class="Experience">
      <header>
        <h1 style="color:#008B8B;"> Experience</h1>
      </header>
      <ul style="padding-left:2em">
        <li>
          <p>
            Research Intern (Remote), Shanghai Artificial Intelligence Laboratory (Aug. 2024 -- Present)
          </p>
        </li>

        <li>
          <p>
            Research Assistant, Department of Computer Science and Technology, Tsinghua University (Aug. 2023 -- Jul. 2024)
          </p>
        </li>

        <li>
          <p>
            Research Intern, Institute for AI Industry Research, Tsinghua University (Nov. 2023 -- Apr. 2024)
          </p>
        </li>

        <li>
          <p>
            Research Intern, Microsoft Research Asia (Nov. 2022 -- Nov. 2023)
          </p>
        </li>

      </ul>
    </div>

    <div class="Services">
      <header>
        <h1 style="color:#008B8B;"> Services</h1>
      </header>
      <ul style="padding-left:2em">
        <li>
          <p>
            Reviewer (Program Committee): CVPR (2023, 2024, 2025), ECCV (2022, 2024), ICCV (2025), ACM MM (2024), AAAI (2024), ICLR (2024, 2025), NeurIPS (2023, 2024), ICML (2025), ECAI (2023); Neural Networks
          </p>
        </li>
        <li>
          <p>
            IEEE Standards Association: Member of IEEE C/DC 3161 Working Group (IEEE Digital Retina Systems)
          </p>
        </li>
    </div>

    <div class="Rewards">
      <header>
        <h1 style="color:#008B8B;"> Selected Rewards</h1>
      </header>
      <ul style="padding-left:2em">
        <li>
          <p>
            Distinguished Master's Thesis Award, Tsinghua University, 2023
          </p>
        </li>
        <li>
          <p>
            Internship Award (Second Prize), Tsinghua University, 2023
          </p>
        </li>
        <li>
          <p>
            First Prize Scholarship (Huiyan Scholarship), Tsinghua University, 2022
          </p>
        </li>
        <li>
          <p>
            Star of Tomorrow Excellent Internship Award, Microsoft Research Asia, 2022
          </p>
        </li>
        <li>
          <p>
            Merit Undergraduate Student, 2019
          </p>
        </li>
        <li>
          <p>
            Outstanding Undergraduate Student, 2018
          </p>
        </li>
      </ul>
    </div>

    <br>
    <br>
    <p align="right"> Code from <a href="https://ancientmooner.github.io/" target="_blank">Han Hu</a></p>
  </div>

  <script type="text/javascript" src="js/jquery.min.js"></script>
  <script type="text/javascript" src="js/bootstrap.min.js"></script>
  <script type="text/javascript" src="js/script.js"></script>
</body>

</html>